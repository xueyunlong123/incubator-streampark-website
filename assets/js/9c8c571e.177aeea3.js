"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[1368],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=a.createContext({}),l=function(e){var n=a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},u=function(e){var n=l(e.components);return a.createElement(c.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(t),m=r,h=p["".concat(c,".").concat(m)]||p[m]||d[m]||o;return t?a.createElement(h,i(i({ref:n},u),{},{components:t})):a.createElement(h,i({ref:n},u))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=m;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var l=2;l<o;l++)i[l]=t[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},85162:(e,n,t)=>{t.d(n,{Z:()=>i});var a=t(67294),r=t(86010);const o="tabItem_Ymn6";function i(e){let{children:n,hidden:t,className:i}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.Z)(o,i),hidden:t},n)}},74866:(e,n,t)=>{t.d(n,{Z:()=>w});var a=t(87462),r=t(67294),o=t(86010),i=t(12466),s=t(16550),c=t(91980),l=t(67392),u=t(50012);function p(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:n,label:t,attributes:a,default:r}}=e;return{value:n,label:t,attributes:a,default:r}}))}function d(e){const{values:n,children:t}=e;return(0,r.useMemo)((()=>{const e=n??p(t);return function(e){const n=(0,l.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function h(e){let{queryString:n=!1,groupId:t}=e;const a=(0,s.k6)(),o=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c._X)(o),(0,r.useCallback)((e=>{if(!o)return;const n=new URLSearchParams(a.location.search);n.set(o,e),a.replace({...a.location,search:n.toString()})}),[o,a])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,o=d(e),[i,s]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const a=t.find((e=>e.default))??t[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:n,tabValues:o}))),[c,l]=h({queryString:t,groupId:a}),[p,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,o]=(0,u.Nk)(t);return[a,(0,r.useCallback)((e=>{t&&o.set(e)}),[t,o])]}({groupId:a}),g=(()=>{const e=c??p;return m({value:e,tabValues:o})?e:null})();(0,r.useLayoutEffect)((()=>{g&&s(g)}),[g]);return{selectedValue:i,selectValue:(0,r.useCallback)((e=>{if(!m({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);s(e),l(e),f(e)}),[l,f,o]),tabValues:o}}var g=t(72389);const y="tabList__CuJ",b="tabItem_LNqP";function v(e){let{className:n,block:t,selectedValue:s,selectValue:c,tabValues:l}=e;const u=[],{blockElementScrollPositionUntilNextRender:p}=(0,i.o5)(),d=e=>{const n=e.currentTarget,t=u.indexOf(n),a=l[t].value;a!==s&&(p(n),c(a))},m=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=u.indexOf(e.currentTarget)+1;n=u[t]??u[0];break}case"ArrowLeft":{const t=u.indexOf(e.currentTarget)-1;n=u[t]??u[u.length-1];break}}n?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},n)},l.map((e=>{let{value:n,label:t,attributes:i}=e;return r.createElement("li",(0,a.Z)({role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,key:n,ref:e=>u.push(e),onKeyDown:m,onClick:d},i,{className:(0,o.Z)("tabs__item",b,i?.className,{"tabs__item--active":s===n})}),t??n)})))}function k(e){let{lazy:n,children:t,selectedValue:a}=e;const o=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=o.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},o.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==a}))))}function S(e){const n=f(e);return r.createElement("div",{className:(0,o.Z)("tabs-container",y)},r.createElement(v,(0,a.Z)({},e,n)),r.createElement(k,(0,a.Z)({},e,n)))}function w(e){const n=(0,g.Z)();return r.createElement(S,(0,a.Z)({key:String(n)},e))}},4750:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>m,frontMatter:()=>s,metadata:()=>l,toc:()=>p});var a=t(87462),r=(t(67294),t(3905)),o=t(74866),i=t(85162);const s={id:"Hbase-Connector",title:"Apache HBase Connector",sidebar_position:6},c=void 0,l={unversionedId:"connector/Hbase-Connector",id:"connector/Hbase-Connector",title:"Apache HBase Connector",description:"Apache HBase is a highly reliable, high-performance, column-oriented, and scalable distributed storage system. Using HBase technology,",source:"@site/docs/connector/6-hbase.md",sourceDirName:"connector",slug:"/connector/Hbase-Connector",permalink:"/docs/connector/Hbase-Connector",draft:!1,editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/docs/connector/6-hbase.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{id:"Hbase-Connector",title:"Apache HBase Connector",sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Elasticsearch Connector",permalink:"/docs/connector/Elasticsearch-Connector"},next:{title:"Http Connector",permalink:"/docs/connector/Http-Connector"}},u={},p=[{value:"Dependency of HBase writing",id:"dependency-of-hbase-writing",level:2},{value:"Regular way to write and read Hbase",id:"regular-way-to-write-and-read-hbase",level:2},{value:"1.Create database and table",id:"1create-database-and-table",level:3},{value:"2.Write demo and Read demo",id:"2write-demo-and-read-demo",level:3},{value:"write and read Hbase with StreamPark",id:"write-and-read-hbase-with-streampark",level:2},{value:"1. Configure policies and connection information",id:"1-configure-policies-and-connection-information",level:3},{value:"2. Read and write HBase",id:"2-read-and-write-hbase",level:3},{value:"Other configuration",id:"other-configuration",level:2}],d={toc:p};function m(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://hbase.apache.org/book.html"},"Apache HBase")," is a highly reliable, high-performance, column-oriented, and scalable distributed storage system. Using HBase technology,\nlarge-scale structured storage clusters can be built on cheap PC Servers. Unlike general relational databases,\nHBase is a database suitable for unstructured data storage because HBase storage is based on a column rather than a row-based schema."),(0,r.kt)("p",null,"Flink does not officially provide a connector for Hbase DataStream. StreamPark encapsulates HBaseSource and HBaseSink based on ",(0,r.kt)("inlineCode",{parentName:"p"},"Hbase-client"),".\nIt supports automatic connection creation based on configuration and simplifies development. StreamPark reading Hbase can record the latest status of the read data when the checkpoint is enabled,\nand the offset corresponding to the source can be restored through the data itself. Implement source-side AT_LEAST_ONCE."),(0,r.kt)("p",null,"HbaseSource implements Flink Async I/O to improve streaming throughput. The sink side supports AT_LEAST_ONCE by default.\nEXACTLY_ONCE is supported when checkpointing is enabled."),(0,r.kt)("admonition",{title:"hint",type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"StreamPark reading HBASE can record the latest state of the read data when checkpoint is enabled.\nWhether the previous state can be restored after the job is resumed depends entirely on whether the data itself has an offset identifier,\nwhich needs to be manually specified in the code. The recovery logic needs to be specified in the func parameter of the getDataStream method of HBaseSource.")),(0,r.kt)("h2",{id:"dependency-of-hbase-writing"},"Dependency of HBase writing"),(0,r.kt)("p",null,"HBase Maven Dependency"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<dependency>\n<groupId>org.apache.hbase</groupId>\n<artifactId>hbase-client</artifactId>\n<version>${hbase.version}</version>\n</dependency>\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"\n<dependency>\n<groupId>org.apache.hbase</groupId>\n<artifactId>hbase-common</artifactId>\n<version>${hbase.version}</version>\n</dependency>\n")),(0,r.kt)("h2",{id:"regular-way-to-write-and-read-hbase"},"Regular way to write and read Hbase"),(0,r.kt)("h3",{id:"1create-database-and-table"},"1.Create database and table"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"}," create 'Student', {NAME => 'Stulnfo', VERSIONS => 3}, {NAME =>'Grades', BLOCKCACHE => true}\n")),(0,r.kt)("h3",{id:"2write-demo-and-read-demo"},"2.Write demo and Read demo"),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"read data",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.functions.source.RichSourceFunction;\nimport org.apache.hadoop.hbase.Cell;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.apache.hadoop.hbase.util.Bytes;\n\nimport java.util.List;\n\n\npublic class FlinkHBaseReader {\n    public static void main(String[] args) throws Exception {\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        // \u6dfb\u52a0\u6570\u636e\u6e90\n        DataStream<String> stream = env.addSource(new HBaseReader());\n        stream.print();\n        env.execute("FlinkHBaseDemo");\n    }\n}\n\n\nclass HBaseReader extends RichSourceFunction<String> {\n    private Connection connection = null;\n    private ResultScanner rs = null;\n    private Table table = null;\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        org.apache.hadoop.conf.Configuration hconf = HBaseConfiguration.create();\n        hconf.set("hbase.zookeeper.quorum", "localhost:2181");\n        hconf.set("zookeeper.property.clientPort", "/hbase");\n        connection = ConnectionFactory.createConnection(hconf);\n    }\n\n    @Override\n    public void run(SourceContext<String> sourceContext) throws Exception {\n        table = connection.getTable(TableName.valueOf("Student"));\n        Scan scan = new Scan();\n        scan.addFamily(Bytes.toBytes("Stulnfo"));\n        rs = table.getScanner(scan);\n        for (Result result : rs) {\n            StringBuilder sb = new StringBuilder();\n            List<Cell> cells = result.listCells();\n            for (Cell cell : cells) {\n                String value = Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());\n                sb.append(value).append("-");\n            }\n            String value = sb.replace(sb.length() - 1, sb.length(), "").toString();\n            sourceContext.collect(value);\n        }\n    }\n\n    @Override\n    public void cancel() {\n\n    }\n\n    @Override\n    public void close() throws Exception {\n        if (rs != null) {\n            rs.close();\n        }\n        if (table != null) {\n            table.close();\n        }\n        if (connection != null) {\n            connection.close();\n        }\n    }\n}\n'))),(0,r.kt)(i.Z,{value:"data input",default:!0,mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-java"},'import com.zhisheng.common.utils.ExecutionEnvUtil;\nimport lombok.extern.slf4j.Slf4j;\nimport org.apache.flink.api.java.utils.ParameterTool;\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\nimport org.apache.flink.streaming.api.functions.source.SourceFunction;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.*;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\n\n/**\n * Desc: Read stream data, then write to HBase\n */\n@Slf4j\npublic class HBaseStreamWriteMain {\n\n    public static void main(String[] args) throws Exception {\n        final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);\n        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);\n        DataStream<String> dataStream = env.addSource(new SourceFunction<String>() {\n            private static final long serialVersionUID = 1L;\n\n            private volatile boolean isRunning = true;\n\n            @Override\n            public void run(SourceContext<String> out) throws Exception {\n                while (isRunning) {\n                    out.collect("name" + Math.floor(Math.random() * 100));\n                }\n            }\n\n            @Override\n            public void cancel() {\n                isRunning = false;\n            }\n        });\n        dataStream.addSink(new HBaseWriter());\n        env.execute("Flink HBase connector sink");\n    }\n\n\n}\n\n/**\nWrite to HBase\n  Inherit RichSinkFunction to override the parent class method\n  <p>\n  When writing to hbase, 500 items are flushed once, inserted in batches, using writeBufferSize\n */\nclass HBaseWriter extends RichSinkFunction<String> {\n    private static final Logger logger = LoggerFactory.getLogger(HBaseWriter.class);\n\n    private static org.apache.hadoop.conf.Configuration configuration;\n    private static Connection connection = null;\n    private static BufferedMutator mutator;\n    private static int count = 0;\n\n    @Override\n    public void open(Configuration parameters) throws Exception {\n        configuration = HBaseConfiguration.create();\n        configuration.set("hbase.zookeeper.quorum", "localhost:21981");\n        configuration.set("zookeeper.property.clientPort", "/hbase");\n        try {\n            connection = ConnectionFactory.createConnection(configuration);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n        BufferedMutatorParams params = new BufferedMutatorParams(TableName.valueOf("Student"));\n        params.writeBufferSize(2 * 1024 * 1024);\n        mutator = connection.getBufferedMutator(params);\n    }\n\n    @Override\n    public void close() throws IOException {\n        if (mutator != null) {\n            mutator.close();\n        }\n        if (connection != null) {\n            connection.close();\n        }\n    }\n\n    @Override\n    public void invoke(String values, Context context) throws Exception {\n        //Date 1970-01-06 11:45:55  to 445555000\n        long unixTimestamp = 0;\n        String RowKey = String.valueOf(unixTimestamp);\n        Put put = new Put(RowKey.getBytes());\n        put.addColumn("Stulnfo".getBytes(), "Name".getBytes(), values.getBytes());\n        mutator.mutate(put);\n        //\u6bcf\u6ee1500\u6761\u5237\u65b0\u4e00\u4e0b\u6570\u636e\n        if (count >= 500) {\n            mutator.flush();\n            count = 0;\n        }\n        count = count + 1;\n    }\n}\n')))),(0,r.kt)("p",null,"Reading and writing HBase in this way is cumbersome and inconvenient. ",(0,r.kt)("inlineCode",{parentName:"p"},"StreamPark")," follows the concept of convention over configuration and automatic configuration.\nUsers only need to configure Hbase connection parameters and Flink operating parameters. StreamPark will automatically assemble source and sink,\nwhich greatly simplifies development logic and improves development efficiency and maintainability."),(0,r.kt)("h2",{id:"write-and-read-hbase-with-streampark"},"write and read Hbase with StreamPark"),(0,r.kt)("h3",{id:"1-configure-policies-and-connection-information"},"1. Configure policies and connection information"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"# hbase\nhbase:\n  zookeeper.quorum: test1,test2,test6\n  zookeeper.property.clientPort: 2181\n  zookeeper.session.timeout: 1200000\n  rpc.timeout: 5000\n  client.pause: 20\n\n")),(0,r.kt)("h3",{id:"2-read-and-write-hbase"},"2. Read and write HBase"),(0,r.kt)("p",null,"Writing to Hbase with StreamPark is very simple, the code is as follows:"),(0,r.kt)(o.Z,{mdxType:"Tabs"},(0,r.kt)(i.Z,{value:"read HBase",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'\nimport org.apache.streampark.common.util.ConfigUtils\nimport org.apache.streampark.flink.core.java.wrapper.HBaseQuery\nimport org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.request.HBaseRequest\nimport org.apache.streampark.flink.core.scala.source.HBaseSource\nimport org.apache.flink.api.scala.createTypeInformation\nimport org.apache.hadoop.hbase.CellUtil\nimport org.apache.hadoop.hbase.client.{Get, Scan}\nimport org.apache.hadoop.hbase.util.Bytes\n\nimport java.util\n\nobject HBaseSourceApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n\n    implicit val conf = ConfigUtils.getHBaseConfig(context.parameter.toMap)\n\n    val id = HBaseSource().getDataStream[String](query => {\n        new HBaseQuery("person", new Scan())\n    },\n      //The following methods determine the logic for restoring offsets from checkpoints\n      r => new String(r.getRow), null)\n     //flink Async I/O\n    HBaseRequest(id).requestOrdered(x => {\n      new HBaseQuery("person", new Get(x.getBytes()))\n    }, (a, r) => {\n      val map = new util.HashMap[String, String]()\n      val cellScanner = r.cellScanner()\n      while (cellScanner.advance()) {\n        val cell = cellScanner.current()\n        val q = Bytes.toString(CellUtil.cloneQualifier(cell))\n        val (name, v) = q.split("_") match {\n          case Array(_type, name) =>\n            _type match {\n              case "i" => name -> Bytes.toInt(CellUtil.cloneValue(cell))\n              case "s" => name -> Bytes.toString(CellUtil.cloneValue(cell))\n              case "d" => name -> Bytes.toDouble(CellUtil.cloneValue(cell))\n              case "f" => name -> Bytes.toFloat(CellUtil.cloneValue(cell))\n            }\n          case _ =>\n        }\n        map.put(name.toString, v.toString)\n      }\n      map.toString\n    }).print("Async")\n  }\n\n}\n\n'))),(0,r.kt)(i.Z,{value:"write HBase",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'import org.apache.streampark.flink.core.scala.FlinkStreaming\nimport org.apache.streampark.flink.core.scala.sink.{HBaseOutputFormat, HBaseSink}\nimport org.apache.flink.api.scala._\nimport org.apache.streampark.common.util.ConfigUtils\nimport org.apache.hadoop.hbase.client.{Mutation, Put}\nimport org.apache.hadoop.hbase.util.Bytes\n\nimport java.util.{Collections, Random}\n\nobject HBaseSinkApp extends FlinkStreaming {\n\n  override def handle(): Unit = {\n    val source = context.addSource(new TestSource)\n    val random = new Random()\n\n    //\u5b9a\u4e49\u8f6c\u6362\u89c4\u5219...\n    implicit def entry2Put(entity: TestEntity): java.lang.Iterable[Mutation] = {\n      val put = new Put(Bytes.toBytes(System.nanoTime() + random.nextInt(1000000)), entity.timestamp)\n      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("cid"), Bytes.toBytes(entity.cityId))\n      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("oid"), Bytes.toBytes(entity.orderId))\n      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("os"), Bytes.toBytes(entity.orderStatus))\n      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("oq"), Bytes.toBytes(entity.quantity))\n      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("sid"), Bytes.toBytes(entity.siteId))\n      Collections.singleton(put)\n    }\n    //source ===> trans ===> sink\n\n    //1\uff09INSERT WAY 1\n    HBaseSink().sink[TestEntity](source, "order")\n    //2) \u63d2\u5165\u65b9\u5f0f2\n    //1.Specify the HBase configuration file\n    implicit val prop = ConfigUtils.getHBaseConfig(context.parameter.toMap)\n    //2.break in...\n    source.writeUsingOutputFormat(new HBaseOutputFormat[TestEntity]("order", entry2Put))\n  }\n}\n')))),(0,r.kt)("p",null,"When StreamPark writes to HBase, you need to create the method of HBaseQuery,\nspecify the method to convert the query result into the required object, identify whether it is running,\nand pass in the running parameters. details as follows"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"/**\n * @param ctx\n * @param property\n */\nclass HBaseSource(@(transient@param) val ctx: StreamingContext, property: Properties = new Properties()) {\n\n  /**\n   * @param query   Specify the method to create H Base Query\n   * @param func    The query results are converted into the expected counterparty method\n   * @param running runID\n   * @param prop    Job parameters\n   * @tparam R\n   * @return\n   */\n  def getDataStream[R: TypeInformation](query: R => HBaseQuery,\n                                        func: Result => R,\n                                        running: Unit => Boolean)(implicit prop: Properties = new Properties()) = {\n    Utils.copyProperties(property, prop)\n    val hBaseFunc = new HBaseSourceFunction[R](prop, query, func, running)\n    ctx.addSource(hBaseFunc)\n  }\n\n}\n")),(0,r.kt)("p",null,"StreamPark HbaseSource implements flink Async I/O, which is used to improve the throughput of Streaming: first create a DataStream,\nthen create an HBaseRequest and call requestOrdered() or requestUnordered() to create an asynchronous stream, as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},"class HBaseRequest[T: TypeInformation](@(transient@param) private val stream: DataStream[T], property: Properties = new Properties()) {\n\n  /**\n   *\n   * @param queryFunc\n   * @param resultFunc\n   * @param timeout\n   * @param capacity\n   * @param prop\n   * @tparam R\n   * @return\n   */\n  def requestOrdered[R: TypeInformation](queryFunc: T => HBaseQuery, resultFunc: (T, Result) => R, timeout: Long = 1000, capacity: Int = 10)(implicit prop: Properties): DataStream[R] = {\n    Utils.copyProperties(property, prop)\n    val async = new HBaseAsyncFunction[T, R](prop, queryFunc, resultFunc, capacity)\n    AsyncDataStream.orderedWait(stream, async, timeout, TimeUnit.MILLISECONDS, capacity)\n  }\n\n  /**\n   *\n   * @param queryFunc\n   * @param resultFunc\n   * @param timeout\n   * @param capacity\n   * @param prop\n   * @tparam R\n   * @return\n   */\n  def requestUnordered[R: TypeInformation](queryFunc: T => HBaseQuery, resultFunc: (T, Result) => R, timeout: Long = 1000, capacity: Int = 10)(implicit prop: Properties): DataStream[R] = {\n    Utils.copyProperties(property, prop)\n    val async = new HBaseAsyncFunction[T, R](prop, queryFunc, resultFunc, capacity)\n    AsyncDataStream.unorderedWait(stream, async, timeout, TimeUnit.MILLISECONDS, capacity)\n  }\n\n}\n")),(0,r.kt)("p",null,"StreamPark supports two ways to write data: 1. addSink() 2. writeUsingOutputFormat Examples are as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-scala"},'    //1\uff09Insert way 1\n    HBaseSink().sink[TestEntity](source, "order")\n    //2) insert way 2\n    //1.Specify the HBase configuration file\n    implicit val prop = ConfigUtils.getHBaseConfig(context.parameter.toMap)\n    //\n    source.writeUsingOutputFormat(new HBaseOutputFormat[TestEntity]("order", entry2Put))\n')),(0,r.kt)("h2",{id:"other-configuration"},"Other configuration"),(0,r.kt)("p",null,"All other configurations must comply with the StreamPark configuration. For specific configurable items and the role of each parameter, please refer to the \u3010project configuration](/docs/development/conf)"))}m.isMDXComponent=!0}}]);