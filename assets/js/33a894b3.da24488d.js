"use strict";(self.webpackChunkapache_streampark_website=self.webpackChunkapache_streampark_website||[]).push([[8188],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(n),u=o,h=m["".concat(l,".").concat(u)]||m[u]||d[u]||r;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},91340:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=n(87462),o=(n(67294),n(3905));const r={slug:"streampark-flink-on-k8s",title:"StreamPark Flink on Kubernetes practice",tags:["StreamPark","Production Practice","FlinkSQL","Kubernetes"],description:"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe"},i=void 0,s={permalink:"/blog/streampark-flink-on-k8s",editUrl:"https://github.com/apache/incubator-streampark-website/edit/dev/blog/0-streampark-flink-on-k8s.md",source:"@site/blog/0-streampark-flink-on-k8s.md",title:"StreamPark Flink on Kubernetes practice",description:"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe",date:"2024-01-21T04:16:34.000Z",formattedDate:"January 21, 2024",tags:[{label:"StreamPark",permalink:"/blog/tags/stream-park"},{label:"Production Practice",permalink:"/blog/tags/production-practice"},{label:"FlinkSQL",permalink:"/blog/tags/flink-sql"},{label:"Kubernetes",permalink:"/blog/tags/kubernetes"}],readingTime:13.97,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"streampark-flink-on-k8s",title:"StreamPark Flink on Kubernetes practice",tags:["StreamPark","Production Practice","FlinkSQL","Kubernetes"],description:"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe"},nextItem:{title:"StreamPark - Powerful Flink Development Framework",permalink:"/blog/flink-development-framework-streampark"}},l={authorsImageUrls:[]},p=[{value:"<strong>Why Choose Native Kubernetes</strong>",id:"why-choose-native-kubernetes",level:2},{value:"<strong>Deploy Flink on Kubernetes using StreamPark</strong>",id:"deploy-flink-on-kubernetes-using-streampark",level:2},{value:"<strong>Basic environment configuration</strong>",id:"basic-environment-configuration",level:3},{value:"<strong>Job development</strong>",id:"job-development",level:3},{value:"<strong>Job online</strong>",id:"job-online",level:3},{value:"<strong>Assignment submission</strong>",id:"assignment-submission",level:3},{value:"<strong>Job management</strong>",id:"job-management",level:3},{value:"<strong>StreamPark\u2019s implementation in Wuxin Technology</strong>",id:"streamparks-implementation-in-wuxin-technology",level:2},{value:"Problems encountered",id:"problems-encountered",level:2},{value:"<strong>FAQs are summarized below</strong>",id:"faqs-are-summarized-below",level:3},{value:"<strong>Best Practices</strong>",id:"best-practices",level:3},{value:"<strong>Future Expectations</strong>",id:"future-expectations",level:2}],c={toc:p};function m(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,a.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Wuxin Technology was founded in January 2018. The current main business includes the research and development, design, manufacturing and sales of RELX brand products. With core technologies and capabilities covering the entire industry chain, RELX is committed to providing users with products that are both high quality and safe."),(0,o.kt)("h2",{id:"why-choose-native-kubernetes"},(0,o.kt)("strong",{parentName:"h2"},"Why Choose Native Kubernetes")),(0,o.kt)("p",null,"Native Kubernetes offers the following advantages:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Shorter Failover time")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Resource hosting can be implemented without the need to manually create TaskManager Pods, which can be automatically destroyed")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"With more convenient HA, in Native Kubernetes mode after Flink version 1.12, you can rely on the Leader election mechanism of native Kubernetes to complete JobManager's HA"),(0,o.kt)("p",{parentName:"li"},"The main difference between Native Kubernetes and Standalone Kubernetes lies in the way Flink interacts with Kubernetes and the resulting series of advantages. Standalone Kubernetes requires users to customize the Kubernetes resource description files of JobManager and TaskManager. When submitting a job, you need to use kubectl combined with the resource description file to start the Flink cluster. The Native Kubernetes mode Flink Client integrates a Kubernetes Client, which can directly communicate with the Kubernetes API Server to complete the creation of JobManager Deployment and ConfigMap. After JobManager Development is created, the Resource Manager module in it can directly communicate with the Kubernetes API Server to complete the creation and destruction of TaskManager pods and the elastic scaling of Taskmanager. Therefore, it is recommended to use Flink on Native Kubernetes mode to deploy Flink tasks in production environments."))),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(22531).Z,width:"1080",height:"401"})),(0,o.kt)("p",null,"When Flink On Kubernetes meets StreamPark"),(0,o.kt)("p",null,"  Flink on Native Kubernetes currently supports Application mode and Session mode. Compared with the two, Application mode deployment avoids the resource isolation problem and client resource consumption problem of Session mode. Therefore, it is recommended to use Application Mode to deploy Flink tasks in ",(0,o.kt)("strong",{parentName:"p"}," production environments. "),"Let\u2019s take a look at the method of using the original script and the process of using StreamPark to develop and deploy a Flink on Native Kubernetes job.\nDeploy Kubernetes using scripts"),(0,o.kt)("p",null,"In the absence of a platform that supports Flink on Kubernetes task development and deployment, you need to use scripts to submit and stop tasks. This is also the default method provided by Flink. The specific steps are as follows:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Prepare the kubectl and Docker command running environment on the Flink client node, create the Kubernetes Namespace and Service Account used to deploy the Flink job, and perform RBAC"),(0,o.kt)("li",{parentName:"ol"},"Write a Dockerfile file to package the Flink base image and the user\u2019s job Jar together")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-dockerfile"},"\nFROM flink:1.13.6-scala_2.11\nRUN mkdir -p $FLINK_HOME/usrlib\nCOPY my-flink-job.jar $FLINK_HOME/usrlib/my-flink-job.jar\n")),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},"Use Flink client script to start Flink tasks")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"\n./bin/flink run-application \\\n    --target kubernetes-application \\\n    -Dkubernetes.namespace=flink-cluster \\\n    -Dkubernetes.jobmanager.service-account=default \\\n    -Dkubernetes.cluster-id=my-first-application-cluster \\\n    -Dkubernetes.container.image=relx_docker_url/streamx/relx_flink_1.13.6-scala_2.11:latest \\\n    -Dkubernetes.rest-service.exposed.type=NodePort \\\n    local:///opt/flink/usrlib/my-flink-job.jar\n")),(0,o.kt)("ol",{start:4},(0,o.kt)("li",{parentName:"ol"},"Use the Kubectl command to obtain the WebUI access address and JobId of the Flink job.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"kubectl -n flink-cluster get svc\n")),(0,o.kt)("ol",{start:5},(0,o.kt)("li",{parentName:"ol"},"Stop the job using Flink command")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/flink cancel\n    --target kubernetes-application\n    -Dkubernetes.cluster-id=my-first-application-cluster\n    -Dkubernetes.namespace=flink-cluster <jobId>\n")),(0,o.kt)("p",null,"  The above is the process of deploying a Flink task to Kubernetes using the most original script method provided by Flink. Only the most basic task submission is achieved. If it is to reach the production use level, there are still a series of problems that need to be solved, such as: the method is too Originally, it was unable to adapt to large batches of tasks, unable to record task checkpoints and real-time status tracking, difficult to operate and monitor tasks, had no alarm mechanism, and could not be managed in a centralized manner, etc."),(0,o.kt)("h2",{id:"deploy-flink-on-kubernetes-using-streampark"},(0,o.kt)("strong",{parentName:"h2"},"Deploy Flink on Kubernetes using StreamPark")),(0,o.kt)("p",null,"  There will be higher requirements for using Flink on Kubernetes in enterprise-level production environments. Generally, you will choose to build your own platform or purchase related commercial products. No matter which solution meets the product capabilities: large-scale task development and deployment, status tracking, operation and maintenance monitoring , failure alarms, unified task management, high availability, etc. are common demands."),(0,o.kt)("p",null,"  In response to the above issues, we investigated open source projects in the open source field that support the development and deployment of Flink on Kubernetes tasks. During the investigation, we also encountered other excellent open source projects. After comprehensively comparing multiple open source projects, we came to the conclusion: ",(0,o.kt)("strong",{parentName:"p"},"StreamPark has great performance in either completness, user experience, or stability, so we finally chose StreamPark as our one-stop real-time computing platform. ")),(0,o.kt)("p",null,"  Let\u2019s take a look at how StreamPark supports Flink on Kubernetes:"),(0,o.kt)("h3",{id:"basic-environment-configuration"},(0,o.kt)("strong",{parentName:"h3"},"Basic environment configuration")),(0,o.kt)("p",null,"  Basic environment configuration includes Kubernetes and Docker repository information as well as Flink client information configuration. The simplest way for the Kubernetes basic environment is to directly copy the .kube/config of the Kubernetes node to the StreamPark node user directory, and then use the kubectl command to create a Flink-specific Kubernetes Namespace and perform RBAC configuration."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"# Create k8s namespace used by Flink jobs\nkubectl create ns flink-cluster\n# Bind RBAC resources to the default user\nkubectl create clusterrolebinding flink-role-binding-default --clusterrole=edit --serviceaccount=flink-cluster:default\n\n")),(0,o.kt)("p",null,"Docker account information can be configured directly in the Docker Setting interface:"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(99171).Z,width:"1080",height:"586"})),(0,o.kt)("p",null,"StreamPark can adapt to multi-version Flink job development. The Flink client can be configured directly on the StreamPark Setting interface:"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(26601).Z,width:"1080",height:"352"})),(0,o.kt)("h3",{id:"job-development"},(0,o.kt)("strong",{parentName:"h3"},"Job development")),(0,o.kt)("p",null,"After StreamPark has configured the basic environment, it only takes three steps to develop and deploy a Flink job:"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(16836).Z,width:"1080",height:"271"})),(0,o.kt)("p",null,"  StreamPark supports both Upload Jar and direct writing of Flink SQL jobs. ",(0,o.kt)("strong",{parentName:"p"},"Flink SQL jobs only need to enter SQL and dependencies. This method greatly improves the development experience and avoids problems such as dependency conflicts.")," This article does not focus on this part\u3002"),(0,o.kt)("p",null,"  Here you need to select the deployment mode as kubernetes application, and configure the following parameters on the job development page: The parameters in the red box are the basic parameters of Flink on Kubernetes."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(62322).Z,width:"1080",height:"1104"})),(0,o.kt)("p",null,"  The following parameters are parameters related to Flink jobs and resources. The choice of Resolve Order is related to the code loading mode. For jobs uploaded by the Upload Jar developed by the DataStream API, choose to use Child-first, and for Flink SQL jobs, choose to use Parent-first loading."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(52258).Z,width:"1080",height:"1133"})),(0,o.kt)("p",null,"  Finally, there are the following two heavyweight parameters. For Native Kubernetes, k8s-pod-template generally only requires pod-template configuration. Dynamic Option is a supplement to the pod-template parameters. For some personalized configurations, you can Configured in Dynamic Option. For more Dynamic Option, please directly refer to the Flink official website."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(26212).Z,width:"1080",height:"1104"})),(0,o.kt)("h3",{id:"job-online"},(0,o.kt)("strong",{parentName:"h3"},"Job online")),(0,o.kt)("p",null,"After the job development is completed, the job comes online. In this step, StreamPark has done a lot of work, as follows:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Prepare environment"),(0,o.kt)("li",{parentName:"ul"},"Dependency download in job"),(0,o.kt)("li",{parentName:"ul"},"Build job (JAR package)"),(0,o.kt)("li",{parentName:"ul"},"Build image"),(0,o.kt)("li",{parentName:"ul"},"Push the image to the remote repository")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"For users: Just click the cloud-shaped online button in the task list")),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(51823).Z,width:"1080",height:"573"})),(0,o.kt)("p",null,"We can see a series of work done by StreamPark when building and pushing the image: ",(0,o.kt)("strong",{parentName:"p"},"Read the configuration, build the image, and push the image to the remote repository...")," I want to give StreamPark a big thumbs up!"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(51008).Z,width:"948",height:"1866"})),(0,o.kt)("h3",{id:"assignment-submission"},(0,o.kt)("strong",{parentName:"h3"},"Assignment submission")),(0,o.kt)("p",null,"  Finally, you only need to click the start Application button in Operation to start a Flink on Kubernetes job. After the job is successfully started, click on the job name to jump to the Jobmanager WebUI page. The whole process is very simple and smooth."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(96807).Z,width:"1080",height:"698"})),(0,o.kt)("p",null,"  The entire process only requires the above three steps to complete the development and deployment of a Flink on Kubernetes job on StreamPark. StreamPark's support for Flink on Kubernetes goes far beyond simply submitting a task."),(0,o.kt)("h3",{id:"job-management"},(0,o.kt)("strong",{parentName:"h3"},"Job management")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"After the job is submitted, StreamPark can obtain the latest checkpoint address of the task, the running status of the task, and the real-time resource consumption information of the cluster in real time. It can very conveniently start and stop the running task with one click, and supports recording the savepoint location when stopping the job. , as well as functions such as restoring the state from savepoint when restarting, thus ensuring the data consistency of the production environment and truly possessing the one-stop development, deployment, operation and maintenance monitoring capabilities of Flink on Kubernetes.")),(0,o.kt)("p",null,"Next, let\u2019s take a look at how StreamPark supports this capability:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Record checkpoint in real time")),(0,o.kt)("p",{parentName:"li"},"After the job is submitted, sometimes it is necessary to change the job logic but to ensure data consistency, then the platform needs to have the ability to record the location of each checkpoint in real time, as well as the ability to record the last savepoint location when stopped. StreamPark is on Flink on Kubernetes This function is implemented very well. By default, checkpoint information will be obtained and recorded in the corresponding table every 5 seconds, and according to the policy of retaining the number of checkpoints in Flink, only state.checkpoints.num-retained will be retained, and the excess will be deleted. There is an option to check the savepoint when the task is stopped. If the savepoint option is checked, the savepoint operation will be performed when the task is stopped, and the specific location of the savepoint will also be recorded in the table."),(0,o.kt)("p",{parentName:"li"},"The root path of the default savepoint only needs to be configured in the Flink Home flink-conf.yaml file to automatically identify it. In addition to the default address, the root path of the savepoint can also be customized and specified when stopping."))),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(26861).Z,width:"1080",height:"446"})),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(41380).Z,width:"1080",height:"479"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Track running status in real time")),(0,o.kt)("p",{parentName:"li"},"For challenges in the production environment, a very important point is whether monitoring is in place, especially for Flink on Kubernetes. This is very important and is the most basic capability. StreamPark can monitor the running status of Flink on Kubernetes jobs in real time and display it to users on the platform. Tasks can be easily retrieved based on various running statuses on the page."))),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(79208).Z,width:"1080",height:"617"})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Complete alarm mechanism")),(0,o.kt)("p",{parentName:"li"},"In addition, StreamPark also has complete alarm functions: supporting email, DingTalk, WeChat and SMS, etc. This is also an important reason why the company chose StreamPark as the one-stop platform for Flink on Kubernetes after initial research."))),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(20676).Z,width:"1080",height:"393"})),(0,o.kt)("p",null,"  From the above, we can see that StreamPark has the capabilities to support the development and deployment process of Flink on Kubernetes, including: ",(0,o.kt)("strong",{parentName:"p"}," job development capabilities, deployment capabilities, monitoring capabilities, operation and maintenance capabilities, exception handling capabilities, etc. StreamPark provides a relatively complete set of s solution. And it already has some CICD/DevOps capabilities, and the overall completion level continues to improve. It is a product that supports the full link of Flink on Kubernetes one-stop development, deployment, operation and maintenance work in the entire open source field. StreamPark is worthy of praise. ")),(0,o.kt)("h2",{id:"streamparks-implementation-in-wuxin-technology"},(0,o.kt)("strong",{parentName:"h2"},"StreamPark\u2019s implementation in Wuxin Technology")),(0,o.kt)("p",null,"  StreamPark was launched late in Wuxin Technology. It is currently mainly used for the development and deployment of real-time data integration jobs and real-time indicator calculation jobs. There are Jar tasks and Flink SQL tasks, all deployed using Native Kubernetes; data sources include CDC, Kafka, etc., and Sink end There are Maxcompute, kafka, Hive, etc. The following is a screenshot of the company's development environment StreamPark platform:"),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(61354).Z,width:"1080",height:"706"})),(0,o.kt)("h2",{id:"problems-encountered"},"Problems encountered"),(0,o.kt)("p",null,"  Any new technology has a process of exploration and fall into pitfalls. The experience of failure is precious. Here are some pitfalls and experiences that StreamPark has stepped into during the implementation of fog core technology. ",(0,o.kt)("strong",{parentName:"p"},"The content of this section is not only about StreamPark. I believe it will bring some reference to all friends who use Flink on Kubernetes"),"."),(0,o.kt)("h3",{id:"faqs-are-summarized-below"},(0,o.kt)("strong",{parentName:"h3"},"FAQs are summarized below")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Kubernetes pod failed to pull the image")),(0,o.kt)("p",{parentName:"li"},"The main problem is that Kubernetes pod-template lacks docker\u2019s imagePullSecrets")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Scala version inconsistent")),(0,o.kt)("p",{parentName:"li"},"Since StreamPark deployment requires a Scala environment, and Flink SQL operation requires the Flink SQL Client provided by StreamPark, it is necessary to ensure that the Scala version of the Flink job is consistent with the Scala version of StreamPark.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Be aware of class conflicts")),(0,o.kt)("p",{parentName:"li"},"When developing Flink SQL jobs, you need to pay attention to whether there are any class conflicts between the Flink image and the Flink connector and UDF. The best way to avoid class conflicts is to make the Flink image and the commonly used Flink connector and user UDF into a usable basic image. After that, other Flink SQL jobs can be reused directly.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"How to store checkpoint without Hadoop environment?")),(0,o.kt)("p",{parentName:"li"},"HDFS, Alibaba Cloud OSS/AWS S3 can both perform checkpoint and savepoint storage. The Flink basic image already has support for OSS and S3. If you do not have HDFS, you can use Alibaba Cloud OSS or S3 to store status and checkpoint and savepoint data. You only need to use Flink Simply configure it in the dynamic parameters."))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"\n-Dstate.backend=rocksdb\n-Dcontainerized.master.env.ENABLE_BUILT_IN_PLUGINS=flink-oss-fs-hadoop-1.13.6.jar\n-Dcontainerized.taskmanager.env.ENABLE_BUILT_IN_PLUGINS=flink-oss-fs-hadoop-1.13.6.jar\n-Dfs.oss.endpoint=xxyy.aliyuncs.com\n-Dfs.oss.accessKeyId=xxxxxxxxxx\n-Dfs.oss.accessKeySecret=xxxxxxxxxx\n-Dstate.checkpoints.dir=oss://realtime-xxx/streamx/dev/checkpoints/\n-Dstate.savepoints.dir=oss://realtime-xxx/streamx/dev/savepoints/\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"The changed code did not take effect after it was republished")),(0,o.kt)("p",{parentName:"li"},"This issue is related to the Kubernetes pod image pull policy. It is recommended to set the Pod image pull policy to Always:"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"\u200d-Dkubernetes.container.image.pull-policy=Always\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Each restart of the task will result in one more Job instance")),(0,o.kt)("p",{parentName:"li"},"Under the premise that kubernetes-based HA is configured, when you need to stop the Flink task, you need to use cancel of StreamPark. Do not delete the Deployment of the Flink task directly through the kubernetes cluster. Because Flink's shutdown has its own shutdown process, when deleting a pod, the corresponding configuration files in the Configmap will also be deleted. Direct deletion of the pod will result in the remnants of the Configmap. When a task with the same name is restarted, two identical jobs will appear because at startup, the task will load the remaining configuration files and try to restore the closed task.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"How to implement kubernetes pod domain name access")),(0,o.kt)("p",{parentName:"li"},"Domain name configuration only needs to be configured in pod-template according to Kubernetes resources. I can share with you a pod-template.yaml template that I summarized based on the above issues:"))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-template\nspec:\n  serviceAccount: default\n  containers:\n  - name: flink-main-container\n    image:\n  imagePullSecrets:\n  - name: regsecret\n  hostAliases:\n    - ip: "192.168.0.1"\n      hostnames:\n      - "node1"\n    - ip: "192.168.0.2"\n      hostnames:\n      - "node2"\n    - ip: "192.168.0.3"\n      hostnames:\n      - "node3"\n\n')),(0,o.kt)("h3",{id:"best-practices"},(0,o.kt)("strong",{parentName:"h3"},"Best Practices")),(0,o.kt)("p",null,"  Many of RELX's big data components are based on Alibaba Cloud, such as Maxcompute and Alibaba Cloud Redis. At the same time, our Flink SQL jobs need to use some UDFs. At first, we adopted the method of using Flink Base image + maven dependency + upload udf jar, but in practice we encountered some problems such as version conflicts and class conflicts. At the same time, if it is a large-volume job, the development efficiency of this method is relatively low. Finally, we packaged the commonly used Flink connectors, udf and Flink base image at the company level into a company-level base image. New Flink SQL jobs can directly write Flink SQL after using this base image, which greatly improves development efficiency."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Let\u2019s share a simple step to create a basic image\uff1a")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"1. Prepare the required JAR")),(0,o.kt)("p",null,"Place the commonly used Flink Connector Jar and the user Udf Jar in the same folder lib. The following are some commonly used connector packages in Flink 1.13.6"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-jar"},"bigdata-udxf-1.0.0-shaded.jar\nflink-connector-jdbc_2.11-1.13.6.jar\nflink-sql-connector-kafka_2.11-1.13.6.jar\nflink-sql-connector-mysql-cdc-2.0.2.jar\nhudi-flink-bundle_2.11-0.10.0.jar\nververica-connector-odps-1.13-vvr-4.0.7.jar\nververica-connector-redis-1.13-vvr-4.0.7.jar\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"2. Prepare Dockerfile")),(0,o.kt)("p",null,"Create a Dockerfile file and place the Dockerfile file in the same folder as the above folder"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"FROM flink:1.13.6-scala_2.11COPY lib $FLINK_HOME/lib/\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"3. Create a basic image and push it to a private repository")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"docker login --username=xxxdocker \\\nbuild -t udf_flink_1.13.6-scala_2.11:latest \\\n.docker tag udf_flink_1.13.6-scala_2.11:latest \\\nk8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latestdocker \\\npush k8s-harbor.xxx.com/streamx/udf_flink_1.13.6-scala_2.11:latest\n")),(0,o.kt)("h2",{id:"future-expectations"},(0,o.kt)("strong",{parentName:"h2"},"Future Expectations")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"StreamPark supports Flink job metric monitoring")),(0,o.kt)("p",{parentName:"li"},"It would be great if StreamPark could connect to Flink Metric data and display Flink\u2019s real-time consumption data at every moment on the StreamPark platform.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"StreamPark supports Flink job log persistence")),(0,o.kt)("p",{parentName:"li"},"For Flink deployed to YARN, if the Flink program hangs, we can go to YARN to view the historical logs. However, for Kubernetes, if the program hangs, the Kubernetes pod will disappear and there will be no way to check the logs. Therefore, users need to use tools on Kubernetes for log persistence. It would be better if StreamPark supports the Kubernetes log persistence interface.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Improvement of the problem of too large image")),(0,o.kt)("p",{parentName:"li"},"StreamPark's current image support for Flink on Kubernetes jobs is to combine the basic image and user code into a Fat image and push it to the Docker repository. The problem with this method is that it takes a long time when the image is too large. It is hoped that the basic image can be restored in the future. There is no need to hit the business code together every time, which can greatly improve development efficiency and save costs."))))}m.isMDXComponent=!0},20676:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/alarm-c2104c1839a1b4bb668d48f092f25faa.png"},41380:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/checkpoint-acf7379b24a3bac6695a517b425f466b.png"},16836:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/development_process-476918cfd29159983fe26b36ef487895.png"},99171:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/docker_setting-80acf43cf64fd390e4d50da8830671c0.png"},52258:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/flink_parameters-ff7790882a753bd88fbf5db9b775a0e3.png"},26601:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/flinkversion_setting-b170a43882590683ea0c7f109f909396.png"},96807:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/homework_submit-1f05dacf0fedfd1423f89ca2dec28437.png"},62322:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/kubernetes_base_parameters-1a28fec8d9d3dc57744324db4ef58551.png"},22531:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/nativekubernetes_architecture-ad376f8ae79ab66d90d95742e8335d53.png"},51823:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/operation-067a84b9b5b1491206780076f98e6f8d.png"},26212:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/pod_template-722285f448ec8adc0fa939d0baea2d10.png"},79208:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/run_status-5a663d9169c0bce8f4cfb993db77ae59.png"},26861:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/savepoint-b0288f5293875d156f20dbe768384076.png"},61354:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/screenshot-2906914515b810aadb10db951d4f02bd.png"},51008:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/step_details-301b14f2dbfa9c41f4c0e75a9086f0a4.png"}}]);